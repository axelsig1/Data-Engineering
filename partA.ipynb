{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04162d18-0f55-47a3-8df8-9abaf34f7480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/14 11:22:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"Assignment-3-Axel-Sigstam\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d6fec0-20d0-493e-807b-19e19cc8f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 413.7 MiB)\n",
      "24/03/14 11:22:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 413.7 MiB)\n",
      "24/03/14 11:22:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-72-de1:10005 (size: 32.6 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:19 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 11:22:20 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/14 11:22:20 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/03/14 11:22:20 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:22:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 413.7 MiB)\n",
      "24/03/14 11:22:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 413.7 MiB)\n",
      "24/03/14 11:22:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-72-de1:10005 (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:22:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:22:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240314112218-0091/0 on worker-20240228152247-192.168.2.250-40823 (192.168.2.250:40823) with 2 core(s)\n",
      "24/03/14 11:22:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20240314112218-0091/0 on hostPort 192.168.2.250:40823 with 2 core(s), 1024.0 MiB RAM\n",
      "24/03/14 11:22:21 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:22:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240314112218-0091/0 is now RUNNING\n",
      "24/03/14 11:22:24 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:37450) with ID 0,  ResourceProfileId 0\n",
      "24/03/14 11:22:24 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/03/14 11:22:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10005 with 366.3 MiB RAM, BlockManagerId(0, 192.168.2.250, 10005, None)\n",
      "24/03/14 11:22:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:22:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:22:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2218 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:22:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:26 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48393\n",
      "24/03/14 11:22:26 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 5.965 s\n",
      "24/03/14 11:22:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/14 11:22:26 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 6.059817 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Resumption of the session'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "## A1 ##\n",
    "########\n",
    "\n",
    "# read a file from the HDFS\n",
    "linesEn = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en')\n",
    "linesEn.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b02573-1085-43b3-91e0-54b829022122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-72-de1:10005 (size: 32.6 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:26 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 11:22:26 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/14 11:22:27 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:22:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-72-de1:10005 (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:22:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:22:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 124 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:22:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:27 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:181) finished in 0.154 s\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:181, took 0.162649 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Återupptagande av sessionen'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesSv = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv')\n",
    "linesSv.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe0c4e9-5cae-4e14-85c5-495f314d39e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:27 INFO SparkContext: Starting job: count at /tmp/ipykernel_67839/212818385.py:4\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Got job 2 (count at /tmp/ipykernel_67839/212818385.py:4) with 2 output partitions\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Final stage: ResultStage 2 (count at /tmp/ipykernel_67839/212818385.py:4)\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_67839/212818385.py:4), which has no missing parents\n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.1 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-72-de1:10005 in memory (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[6] at count at /tmp/ipykernel_67839/212818385.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:22:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:22:27 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 2) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/14 11:22:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:28 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:22:28 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 2) in 1117 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:22:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in English transcripts: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 1073 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/14 11:22:31 INFO DAGScheduler: ResultStage 2 (count at /tmp/ipykernel_67839/212818385.py:4) finished in 4.183 s\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Job 2 finished: count at /tmp/ipykernel_67839/212818385.py:4, took 4.196982 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We notice the same first line in both languages. Good!\n",
    "\n",
    "# Count the number of lines in the English transcript\n",
    "line_count_en = linesEn.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of lines in English transcripts:\", line_count_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b7e85f-9cb4-4436-b7fa-774a53a4a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:31 INFO SparkContext: Starting job: count at /tmp/ipykernel_67839/3777411404.py:5\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Got job 3 (count at /tmp/ipykernel_67839/3777411404.py:5) with 2 output partitions\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Final stage: ResultStage 3 (count at /tmp/ipykernel_67839/3777411404.py:5)\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_67839/3777411404.py:5), which has no missing parents\n",
      "24/03/14 11:22:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.1 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at count at /tmp/ipykernel_67839/3777411404.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:22:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:22:31 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/14 11:22:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:32 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 998 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:22:32 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:22:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in Swedish transcripts: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 964 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/14 11:22:36 INFO DAGScheduler: ResultStage 3 (count at /tmp/ipykernel_67839/3777411404.py:5) finished in 4.799 s\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 3 finished: count at /tmp/ipykernel_67839/3777411404.py:5, took 4.806974 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We get:\n",
    "    # Number of lines in English transcripts: 1862234\n",
    "\n",
    "# Count the number of lines in the Swedish transcript\n",
    "line_count_sv = linesEn.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of lines in Swedish transcripts:\", line_count_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6753ffcd-594f-4ff1-a42a-9e78124ef1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line counts are the same for both languages.\n"
     ]
    }
   ],
   "source": [
    "# We got:\n",
    "    # Number of lines in Swedish transcripts: 1862234\n",
    "\n",
    "# Verify if line counts are the same for both languages\n",
    "if line_count_en == line_count_sv:\n",
    "    print(\"Line counts are the same for both languages.\")\n",
    "else:\n",
    "    print(\"Line counts are different for both languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f027d645-29cb-475c-be02-a089650bc277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for English transcripts RDD: 2\n",
      "Number of partitions for Swedish transcripts RDD: 3\n"
     ]
    }
   ],
   "source": [
    "part_en = linesEn.getNumPartitions()\n",
    "part_sv = linesSv.getNumPartitions()\n",
    "\n",
    "# Print the number of partitions for each RDD\n",
    "print(\"Number of partitions for English transcripts RDD:\", part_en)\n",
    "print(\"Number of partitions for Swedish transcripts RDD:\", part_sv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b766f1e-c855-47fe-8aa3-797ef61d4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:36 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[8] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[8] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10005 (size: 5.3 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed English transcript sample:\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.']\n",
      "['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.']\n",
      "['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.']\n",
      "['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.']\n",
      "['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.']\n",
      "['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)']\n",
      "['madam', 'president,', 'on', 'a', 'point', 'of', 'order.']\n",
      "['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.']\n",
      "['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']\n",
      "\n",
      "Preprocessed Swedish transcript sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 45 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:36 INFO DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:181) finished in 0.076 s\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:181, took 0.082061 s\n",
      "24/03/14 11:22:36 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[9] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[9] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:22:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 61 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:36 INFO DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:181) finished in 0.088 s\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:181, took 0.099659 s\n",
      "24/03/14 11:22:36 INFO SparkContext: Starting job: count at /tmp/ipykernel_67839/2539811547.py:26\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Got job 6 (count at /tmp/ipykernel_67839/2539811547.py:26) with 2 output partitions\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Final stage: ResultStage 6 (count at /tmp/ipykernel_67839/2539811547.py:26)\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[10] at count at /tmp/ipykernel_67839/2539811547.py:26), which has no missing parents\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.6 KiB, free 413.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['återupptagande', 'av', 'sessionen']\n",
      "['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.']\n",
      "['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.']\n",
      "['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.']\n",
      "['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.']\n",
      "['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.']\n",
      "['(parlamentet', 'höll', 'en', 'tyst', 'minut.)']\n",
      "['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.']\n",
      "['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.']\n",
      "['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10005 in memory (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[10] at count at /tmp/ipykernel_67839/2539811547.py:26) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:22:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:22:36 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10005 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.3 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10005 in memory (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:37 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:22:38 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 1999 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:22:40 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 9) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:42 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 9) in 2261 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/14 11:22:42 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:42 INFO DAGScheduler: ResultStage 6 (count at /tmp/ipykernel_67839/2539811547.py:26) finished in 5.906 s\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Job 6 finished: count at /tmp/ipykernel_67839/2539811547.py:26, took 5.915394 s\n",
      "24/03/14 11:22:42 INFO SparkContext: Starting job: count at /tmp/ipykernel_67839/2539811547.py:27\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Got job 7 (count at /tmp/ipykernel_67839/2539811547.py:27) with 3 output partitions\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Final stage: ResultStage 7 (count at /tmp/ipykernel_67839/2539811547.py:27)\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[11] at count at /tmp/ipykernel_67839/2539811547.py:27), which has no missing parents\n",
      "24/03/14 11:22:42 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.6 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:42 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:42 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (PythonRDD[11] at count at /tmp/ipykernel_67839/2539811547.py:27) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:22:42 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:22:42 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:22:42 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11) (192.168.2.250, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/03/14 11:22:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10005 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240314112218-0091/1 on worker-20240228152247-192.168.2.250-40823 (192.168.2.250:40823) with 2 core(s)\n",
      "24/03/14 11:22:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20240314112218-0091/1 on hostPort 192.168.2.250:40823 with 2 core(s), 1024.0 MiB RAM\n",
      "24/03/14 11:22:43 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2 for resource profile id: 0)\n",
      "24/03/14 11:22:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240314112218-0091/1 is now RUNNING\n",
      "24/03/14 11:22:45 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/14 11:22:45 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 2896 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "[Stage 7:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English line count match after preprocessing.\n",
      "Swedish line count match after preprocessing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:46 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 433 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/14 11:22:46 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 3327 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/14 11:22:46 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:22:46 INFO DAGScheduler: ResultStage 7 (count at /tmp/ipykernel_67839/2539811547.py:27) finished in 3.351 s\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:22:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Job 7 finished: count at /tmp/ipykernel_67839/2539811547.py:27, took 3.359653 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A2 ##\n",
    "########\n",
    "\n",
    "# Function to preprocess text: lowercase and tokenize\n",
    "def preprocess_text(line):\n",
    "    # Lowercase the text\n",
    "    line = line.lower()\n",
    "    # Tokenize the text (split on space)\n",
    "    tokens = line.split()\n",
    "    return tokens\n",
    "\n",
    "processed_lines_en = linesEn.map(preprocess_text)\n",
    "processed_lines_sv = linesSv.map(preprocess_text)\n",
    "\n",
    "# Collect and print a few preprocessed lines from each RDD for verification\n",
    "print(\"Preprocessed English transcript sample:\")\n",
    "for line in processed_lines_en.take(10):\n",
    "    print(line)\n",
    "\n",
    "print(\"\\nPreprocessed Swedish transcript sample:\")\n",
    "for line in processed_lines_sv.take(10):\n",
    "    print(line)\n",
    "\n",
    "# Verify that line count are the same after preprocessing\n",
    "count_processed_lines_en = processed_lines_en.count()\n",
    "count_processed_lines_sv = processed_lines_sv.count()\n",
    "\n",
    "if count_processed_lines_en == line_count_en:\n",
    "    print(\"English line count match after preprocessing.\")\n",
    "else:\n",
    "    print(\"Warning! English line count doesn't match after preprocessing.\")\n",
    "\n",
    "if count_processed_lines_sv == line_count_sv:\n",
    "    print(\"Swedish line count match after preprocessing.\")\n",
    "else:\n",
    "    print(\"Warning! Swedish line count doesn't match after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe359492-dea8-4699-a94b-6d72aab0e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both languages have matching line count after preprocessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63aabc1d-a3b7-4a5e-8e6a-73a736fc1b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:22:46 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_67839/92222483.py:12\n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.250:10005 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Registering RDD 13 (reduceByKey at /tmp/ipykernel_67839/92222483.py:12) as input to shuffle 0\n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10005 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Got job 8 (sortBy at /tmp/ipykernel_67839/92222483.py:12) with 2 output partitions\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Final stage: ResultStage 9 (sortBy at /tmp/ipykernel_67839/92222483.py:12)\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_67839/92222483.py:12), which has no missing parents\n",
      "24/03/14 11:22:46 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:52638) with ID 1,  ResourceProfileId 0\n",
      "24/03/14 11:22:46 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)\n",
      "24/03/14 11:22:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:22:46 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:22:46 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (PairwiseRDD[13] at reduceByKey at /tmp/ipykernel_67839/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:22:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:22:46 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 13) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/03/14 11:22:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10005 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:22:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10006 with 366.3 MiB RAM, BlockManagerId(1, 192.168.2.250, 10006, None)\n",
      "24/03/14 11:22:47 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:22:49 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 14) (192.168.2.250, executor 1, partition 0, ANY, 7679 bytes) \n",
      "24/03/14 11:22:49 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:22:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:02 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 13) in 15776 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:23:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 14) in 16991 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/14 11:23:06 INFO DAGScheduler: ShuffleMapStage 8 (reduceByKey at /tmp/ipykernel_67839/92222483.py:12) finished in 20.138 s\n",
      "24/03/14 11:23:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:23:06 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:23:06 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "24/03/14 11:23:06 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[16] at sortBy at /tmp/ipykernel_67839/92222483.py:12), which has no missing parents\n",
      "24/03/14 11:23:06 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:06 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:06 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (PythonRDD[16] at sortBy at /tmp/ipykernel_67839/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:23:06 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:23:06 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 15) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:06 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 16) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10005 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:37450\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10006 (size: 6.7 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:52638\n",
      "24/03/14 11:23:06 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 16) in 335 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/14 11:23:06 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 15) in 369 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/14 11:23:06 INFO DAGScheduler: ResultStage 9 (sortBy at /tmp/ipykernel_67839/92222483.py:12) finished in 0.405 s\n",
      "24/03/14 11:23:06 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Job 8 finished: sortBy at /tmp/ipykernel_67839/92222483.py:12, took 20.614701 s\n",
      "24/03/14 11:23:06 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_67839/92222483.py:12\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Got job 9 (sortBy at /tmp/ipykernel_67839/92222483.py:12) with 2 output partitions\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Final stage: ResultStage 11 (sortBy at /tmp/ipykernel_67839/92222483.py:12)\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[17] at sortBy at /tmp/ipykernel_67839/92222483.py:12), which has no missing parents\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10005 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:06 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.1 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:06 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:06 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (PythonRDD[17] at sortBy at /tmp/ipykernel_67839/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:23:06 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:23:06 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 17) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:06 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 18) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10006 in memory (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.7 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10005 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10006 in memory (size: 6.7 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 17) in 232 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 18) in 248 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:07 INFO DAGScheduler: ResultStage 11 (sortBy at /tmp/ipykernel_67839/92222483.py:12) finished in 0.267 s\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Job 9 finished: sortBy at /tmp/ipykernel_67839/92222483.py:12, took 0.274908 s\n",
      "24/03/14 11:23:07 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Registering RDD 19 (sortBy at /tmp/ipykernel_67839/92222483.py:12) as input to shuffle 1\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Got job 10 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Final stage: ResultStage 14 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting ShuffleMapStage 13 (PairwiseRDD[19] at sortBy at /tmp/ipykernel_67839/92222483.py:12), which has no missing parents\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 11.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:07 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (PairwiseRDD[19] at sortBy at /tmp/ipykernel_67839/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Adding task set 13.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 19) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 20) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10006 (size: 7.3 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10005 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 20) in 383 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 19) in 391 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:07 INFO DAGScheduler: ShuffleMapStage 13 (sortBy at /tmp/ipykernel_67839/92222483.py:12) finished in 0.408 s\n",
      "24/03/14 11:23:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:23:07 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:23:07 INFO DAGScheduler: waiting: Set(ResultStage 14)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[22] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:07 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (PythonRDD[22] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 21) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.250:10005 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:37450\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 21) in 157 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: ResultStage 14 (runJob at PythonRDD.scala:181) finished in 0.176 s\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:07 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:181, took 0.610758 s\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.250:10005 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.250:10006 in memory (size: 7.3 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:07 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_67839/92222483.py:19\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.3 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Registering RDD 24 (reduceByKey at /tmp/ipykernel_67839/92222483.py:19) as input to shuffle 2\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Got job 11 (sortBy at /tmp/ipykernel_67839/92222483.py:19) with 3 output partitions\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Final stage: ResultStage 16 (sortBy at /tmp/ipykernel_67839/92222483.py:19)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting ShuffleMapStage 15 (PairwiseRDD[24] at reduceByKey at /tmp/ipykernel_67839/92222483.py:19), which has no missing parents\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 12.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:07 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:07 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 15 (PairwiseRDD[24] at reduceByKey at /tmp/ipykernel_67839/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:23:07 INFO TaskSchedulerImpl: Adding task set 15.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 22) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 23) (192.168.2.250, executor 1, partition 1, ANY, 7679 bytes) \n",
      "24/03/14 11:23:07 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 24) (192.168.2.250, executor 0, partition 2, ANY, 7679 bytes) \n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10005 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:09 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 24) in 2135 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/14 11:23:22 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 23) in 14854 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 22) in 15211 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: ShuffleMapStage 15 (reduceByKey at /tmp/ipykernel_67839/92222483.py:19) finished in 15.234 s\n",
      "24/03/14 11:23:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:23:23 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:23:23 INFO DAGScheduler: waiting: Set(ResultStage 16)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[27] at sortBy at /tmp/ipykernel_67839/92222483.py:19), which has no missing parents\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:23 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 16 (PythonRDD[27] at sortBy at /tmp/ipykernel_67839/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Adding task set 16.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 25) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 26) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 27) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10005 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10006 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:37450\n",
      "24/03/14 11:23:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:52638\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 27) in 337 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 26) in 357 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 25) in 370 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:23 INFO DAGScheduler: ResultStage 16 (sortBy at /tmp/ipykernel_67839/92222483.py:19) finished in 0.395 s\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Job 11 finished: sortBy at /tmp/ipykernel_67839/92222483.py:19, took 15.641554 s\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.2.250:10006 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.2.250:10005 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_16_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:23 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_67839/92222483.py:19\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Got job 12 (sortBy at /tmp/ipykernel_67839/92222483.py:19) with 3 output partitions\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Final stage: ResultStage 18 (sortBy at /tmp/ipykernel_67839/92222483.py:19)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting ResultStage 18 (PythonRDD[28] at sortBy at /tmp/ipykernel_67839/92222483.py:19), which has no missing parents\n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 11.1 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:23 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 18 (PythonRDD[28] at sortBy at /tmp/ipykernel_67839/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 28) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 29) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 30) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10005 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10006 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10005 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.9 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 28) in 334 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 30) in 372 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 29) in 378 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:23 INFO DAGScheduler: ResultStage 18 (sortBy at /tmp/ipykernel_67839/92222483.py:19) finished in 0.392 s\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Job 12 finished: sortBy at /tmp/ipykernel_67839/92222483.py:19, took 0.401632 s\n",
      "24/03/14 11:23:23 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Registering RDD 30 (sortBy at /tmp/ipykernel_67839/92222483.py:19) as input to shuffle 3\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Got job 13 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Final stage: ResultStage 21 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting ShuffleMapStage 20 (PairwiseRDD[30] at sortBy at /tmp/ipykernel_67839/92222483.py:19), which has no missing parents\n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 12.0 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:23 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:23 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:23 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 20 (PairwiseRDD[30] at sortBy at /tmp/ipykernel_67839/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:23:23 INFO TaskSchedulerImpl: Adding task set 20.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 31) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 32) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:23:23 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 33) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10005 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10006 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 33) in 530 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 32) in 538 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 31) in 656 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/14 11:23:24 INFO DAGScheduler: ShuffleMapStage 20 (sortBy at /tmp/ipykernel_67839/92222483.py:19) finished in 0.671 s\n",
      "24/03/14 11:23:24 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:23:24 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:23:24 INFO DAGScheduler: waiting: Set(ResultStage 21)\n",
      "24/03/14 11:23:24 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[33] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:23:24 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:24 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:24 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (PythonRDD[33] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:23:24 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 34) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10006 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:52638\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_18_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10006 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10005 in memory (size: 7.3 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequently Occurring Words in English:\n",
      "the: 3498574\n",
      "of: 1659884\n",
      "to: 1539823\n",
      "and: 1288620\n",
      "in: 1086089\n",
      "that: 797576\n",
      "a: 773812\n",
      "is: 758087\n",
      "for: 534270\n",
      "we: 522879\n",
      "\n",
      "Top 10 Most Frequently Occurring Words in Swedish:\n",
      "att: 1706309\n",
      "och: 1344895\n",
      "i: 1050989\n",
      "det: 924878\n",
      "som: 913302\n",
      "för: 908703\n",
      "av: 738102\n",
      "är: 694389\n",
      "en: 620347\n",
      "vi: 539808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_17_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.5 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 34) in 173 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/14 11:23:24 INFO DAGScheduler: ResultStage 21 (runJob at PythonRDD.scala:181) finished in 0.185 s\n",
      "24/03/14 11:23:24 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:24 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Job 13 finished: runJob at PythonRDD.scala:181, took 0.881611 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A3 ##\n",
    "########\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "words_en = linesEn.flatMap(preprocess_text)\n",
    "words_sv = linesSv.flatMap(preprocess_text)\n",
    "\n",
    "# Compute the 10 most frequently occurring words in English\n",
    "english_top_10_words = words_en \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "# Compute the 10 most frequently occurring words in Swedish\n",
    "swedish_top_10_words = words_sv \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 Most Frequently Occurring Words in English:\")\n",
    "for word, count in english_top_10_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Frequently Occurring Words in Swedish:\")\n",
    "for word, count in swedish_top_10_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb69b82-71ad-45c4-9153-1d5dad47e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasonable results since all the top 10 words for both languages are among the most common words according to\n",
    "# https://en.wikipedia.org/wiki/Most_common_words_in_English, https://larare.at/svenska/moment/lingvistik/vanligaste_orden_i_svenska_spraket.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d73ee2fe-1b28-4ea5-97ad-ceb8b18f97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:23:24 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Got job 14 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6) with 2 output partitions\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Final stage: ResultStage 22 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6)\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Submitting ResultStage 22 (PythonRDD[34] at zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6), which has no missing parents\n",
      "24/03/14 11:23:24 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 8.4 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:24 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:24 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 22 (PythonRDD[34] at zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 11:23:24 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0\n",
      "24/03/14 11:23:24 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 35) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/14 11:23:24 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:26 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/14 11:23:27 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 35) in 2685 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/14 11:23:28 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 36) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:23:28 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10006 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:31 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 36) in 2812 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/14 11:23:31 INFO DAGScheduler: ResultStage 22 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6) finished in 6.228 s\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:31 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Job 14 finished: zipWithIndex at /tmp/ipykernel_67839/3148246711.py:6, took 6.242388 s\n",
      "24/03/14 11:23:31 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Got job 15 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7) with 3 output partitions\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Final stage: ResultStage 23 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7)\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[35] at zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7), which has no missing parents\n",
      "24/03/14 11:23:31 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 8.4 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:31 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:31 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:31 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:31 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 23 (PythonRDD[35] at zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/14 11:23:31 INFO TaskSchedulerImpl: Adding task set 23.0 with 3 tasks resource profile 0\n",
      "24/03/14 11:23:31 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 37) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:23:31 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 38) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/03/14 11:23:31 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 39) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/14 11:23:31 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:31 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10006 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:31 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 39) in 529 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/14 11:23:34 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 38) in 3570 ms on 192.168.2.250 (executor 1) (2/3)\n",
      "24/03/14 11:23:35 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 37) in 3780 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: ResultStage 23 (zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7) finished in 3.803 s\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 15 finished: zipWithIndex at /tmp/ipykernel_67839/3148246711.py:7, took 3.813225 s\n",
      "24/03/14 11:23:35 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.2.250:10006 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_21_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Final stage: ResultStage 24 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting ResultStage 24 (PythonRDD[36] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (PythonRDD[36] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:23:35 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 40) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_20_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10006 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_19_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.9 KiB, free: 413.9 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10006 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 40) in 42 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:35 INFO DAGScheduler: ResultStage 24 (runJob at PythonRDD.scala:181) finished in 0.058 s\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:181, took 0.061548 s\n",
      "24/03/14 11:23:35 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Got job 17 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Final stage: ResultStage 25 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[37] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.3 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[37] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:23:35 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 41) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of keyed lines in English transcripts RDD:\n",
      "[(0, 'resumption'), (1, 'of'), (2, 'the'), (3, 'session'), (4, 'i')]\n",
      "\n",
      "Sample of keyed lines in transcripts of other language RDD:\n",
      "[(0, 'återupptagande'), (1, 'av'), (2, 'sessionen'), (3, 'jag'), (4, 'förklarar')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:23:35 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 41) in 59 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:23:35 INFO DAGScheduler: ResultStage 25 (runJob at PythonRDD.scala:181) finished in 0.074 s\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Job 17 finished: runJob at PythonRDD.scala:181, took 0.078384 s\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A4 ##\n",
    "########\n",
    "\n",
    "# Key the lines by their line number (hint: ZipWithIndex()).\n",
    "key_en = words_en.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "key_sv = words_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "print(\"Sample of keyed lines in English transcripts RDD:\")\n",
    "print(key_en.take(5))\n",
    "print(\"\\nSample of keyed lines in transcripts of other language RDD:\")\n",
    "print(key_sv.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0681e591-63bb-410e-ae94-debcc9351151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:23:35 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Registering RDD 44 (join at /tmp/ipykernel_67839/552352716.py:2) as input to shuffle 4\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Got job 20 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Final stage: ResultStage 29 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting ShuffleMapStage 28 (PairwiseRDD[44] at join at /tmp/ipykernel_67839/552352716.py:2), which has no missing parents\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.9 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on host-192-168-2-72-de1:10005 (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:23:35 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 28 (PairwiseRDD[44] at join at /tmp/ipykernel_67839/552352716.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/14 11:23:35 INFO TaskSchedulerImpl: Adding task set 28.0 with 5 tasks resource profile 0\n",
      "24/03/14 11:23:35 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 44) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7788 bytes) \n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_25_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 192.168.2.250:10006 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 192.168.2.250:10005 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:35 INFO BlockManagerInfo: Removed broadcast_24_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.8 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of joined RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240314112218-0091/2 on worker-20240228152247-192.168.2.250-40823 (192.168.2.250:40823) with 2 core(s)\n",
      "24/03/14 11:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20240314112218-0091/2 on hostPort 192.168.2.250:40823 with 2 core(s), 1024.0 MiB RAM\n",
      "24/03/14 11:23:36 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "24/03/14 11:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240314112218-0091/2 is now RUNNING\n",
      "24/03/14 11:23:38 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:48960) with ID 2,  ResourceProfileId 0\n",
      "24/03/14 11:23:38 INFO ExecutorMonitor: New executor 2 has registered (new total is 3)\n",
      "24/03/14 11:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10007 with 366.3 MiB RAM, BlockManagerId(2, 192.168.2.250, 10007, None)\n",
      "24/03/14 11:23:39 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 45) (192.168.2.250, executor 2, partition 0, ANY, 7788 bytes) \n",
      "24/03/14 11:23:39 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 46) (192.168.2.250, executor 2, partition 2, ANY, 7788 bytes) \n",
      "24/03/14 11:23:39 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.2.250:10007 (size: 8.8 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:39 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 47) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/03/14 11:23:39 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 48) (192.168.2.250, executor 0, partition 4, ANY, 7788 bytes) \n",
      "24/03/14 11:23:39 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:23:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10007 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/14 11:23:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10007 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:24:20 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 48) in 41386 ms on 192.168.2.250 (executor 0) (1/5)\n",
      "24/03/14 11:27:42 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 46) in 243440 ms on 192.168.2.250 (executor 2) (2/5)\n",
      "24/03/14 11:27:45 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 47) in 246154 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/14 11:28:22 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 45) in 283747 ms on 192.168.2.250 (executor 2) (4/5)\n",
      "24/03/14 11:28:24 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 44) in 288783 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/14 11:28:24 INFO DAGScheduler: ShuffleMapStage 28 (join at /tmp/ipykernel_67839/552352716.py:2) finished in 288.802 s\n",
      "24/03/14 11:28:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:28:24 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:28:24 INFO DAGScheduler: waiting: Set(ResultStage 29)\n",
      "24/03/14 11:28:24 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:28:24 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:28:24 INFO DAGScheduler: Submitting ResultStage 29 (PythonRDD[47] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:28:24 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 12.6 KiB, free 413.4 MiB)\n",
      "24/03/14 11:28:24 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:28:24 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:28:24 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:28:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (PythonRDD[47] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:28:24 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:28:24 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 49) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:28:24 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 192.168.2.250:10005 (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:28:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.2.250:37450\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22373135, ('in', 'av')), (22383745, ('subsidiarity,', 'och')), (22385515, ('out', 'fanns')), (22396045, ('to', '-')), (22406655, ('shadow', 'bästa'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:30:29 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 49) in 125459 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:30:29 INFO DAGScheduler: ResultStage 29 (runJob at PythonRDD.scala:181) finished in 125.487 s\n",
      "24/03/14 11:30:29 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:30:29 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:30:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Job 20 finished: runJob at PythonRDD.scala:181, took 414.312768 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join the two RDDs together according to the line number key\n",
    "joined = key_en.join(key_sv)\n",
    "\n",
    "print(\"\\nSample of joined RDD:\")\n",
    "print(joined.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a66911-217e-4e19-a42e-ada7ca2c5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:30:29 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Got job 21 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Final stage: ResultStage 31 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:30:29 INFO DAGScheduler: Submitting ResultStage 31 (PythonRDD[48] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:30:29 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 13.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:30:29 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 413.3 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:30:30 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:30:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (PythonRDD[48] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:30:30 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:30:30 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 50) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on host-192-168-2-72-de1:10005 in memory (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.2.250:10006 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 192.168.2.250:10007 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 192.168.2.250:10006 (size: 7.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.2 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:30:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 192.168.2.250:10005 in memory (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:30:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.2.250:52638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (excluding empty/missing sentences):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19574040, ('fishing', 'det')), (19667300, ('send', 'också')), (19685555, ('certainly', 'befintliga')), (19778815, ('do,', 'förlora')), (19960420, ('to', 'kommandotrupper'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:32:40 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 50) in 130016 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/14 11:32:40 INFO DAGScheduler: ResultStage 31 (runJob at PythonRDD.scala:181) finished in 130.064 s\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:32:40 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:32:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Job 21 finished: runJob at PythonRDD.scala:181, took 130.080336 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter to exclude line pairs that have an empty/missing “corresponding” sentence\n",
    "joined_filtered = joined.filter(lambda x: all(x[1]))\n",
    "\n",
    "print(\"\\nSample of filtered RDD (excluding empty/missing sentences):\")\n",
    "print(joined_filtered.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "422568dd-c60b-4aef-b995-736af097a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:32:40 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Got job 22 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Final stage: ResultStage 33 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Submitting ResultStage 33 (PythonRDD[49] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:32:40 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 13.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:32:40 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 413.4 MiB)\n",
      "24/03/14 11:32:40 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:32:40 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:32:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (PythonRDD[49] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:32:40 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:32:40 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 51) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:32:40 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.2.250:10005 (size: 7.6 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (small number of words per sentence):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:33:21 INFO BlockManagerInfo: Removed broadcast_28_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:33:21 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.2.250:10006 in memory (size: 7.5 KiB, free: 366.2 MiB)\n",
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22400405, ('jointly', 'bör')), (22418660, ('in', 'tredje')), (22511920, ('it', 'strategi')), (22605050, ('the', 'medborgarna')), (22716180, ('structural', 'utvecklingsländerna'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:34:50 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 51) in 130432 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:34:50 INFO DAGScheduler: ResultStage 33 (runJob at PythonRDD.scala:181) finished in 130.452 s\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:34:50 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:34:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Job 22 finished: runJob at PythonRDD.scala:181, took 130.464681 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter to leave only pairs of sentences with a small number of words per sentence\n",
    "MIN_WORDS = 5\n",
    "joined_filtered_small = joined_filtered.filter(lambda x: len(x[1][0].split()) <= MIN_WORDS and len(x[1][1].split()) <= MIN_WORDS)\n",
    "\n",
    "print(\"\\nSample of filtered RDD (small number of words per sentence):\")\n",
    "print(joined_filtered_small.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2203dd5e-0c78-4350-9422-c5ba85be7b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:34:50 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Final stage: ResultStage 35 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[50] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:34:50 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 14.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:34:50 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/14 11:34:50 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:34:50 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:34:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (PythonRDD[50] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:34:50 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:34:50 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 52) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:34:50 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 192.168.2.250:10005 (size: 7.7 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (same number of words in each sentence):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(22364580, ('the', 'förverkligandet')), (22368145, ('this.', 'betraktas')), (22368685, ('ban', 'jag')), (22377010, ('designed', 'innebär')), (22378910, ('third', 'att'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:37:01 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 52) in 130612 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/14 11:37:01 INFO DAGScheduler: ResultStage 35 (runJob at PythonRDD.scala:181) finished in 130.637 s\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:37:01 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Job 23 finished: runJob at PythonRDD.scala:181, took 130.651037 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter to leave only pairs of sentences with the same number of words in each sentence\n",
    "joined_filtered_same_length = joined_filtered_small.filter(lambda x: len(x[1][0].split()) == len(x[1][1].split()))\n",
    "\n",
    "print(\"\\nSample of filtered RDD (same number of words in each sentence):\")\n",
    "print(joined_filtered_same_length.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd84ecf-fa61-4179-aeec-24e63342ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:37:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Got job 24 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Final stage: ResultStage 37 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Submitting ResultStage 37 (PythonRDD[51] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 11:37:01 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 14.6 KiB, free 413.4 MiB)\n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 192.168.2.250:10005 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Removed broadcast_30_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:37:01 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:37:01 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:37:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (PythonRDD[51] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 11:37:01 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "24/03/14 11:37:01 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 53) (192.168.2.250, executor 2, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Removed broadcast_29_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 192.168.2.250:10005 in memory (size: 7.6 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:37:01 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 192.168.2.250:10007 (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:37:01 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 192.168.2.250:48960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of word pairs RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('european', 'medlemsstaterna'), ('covered', 'för'), ('a', 'fördelning'), ('the', 'frågor.'), ('it', 'framtiden.'), ('behalf', 'ifrågasätter'), ('been', 'amsterdamfördraget.'), ('ethnic', 'vi'), ('response', 'böter'), ('that,', 'en')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:39:07 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 53) in 125931 ms on 192.168.2.250 (executor 2) (1/1)\n",
      "24/03/14 11:39:07 INFO DAGScheduler: ResultStage 37 (runJob at PythonRDD.scala:181) finished in 125.955 s\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:39:07 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:39:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Job 24 finished: runJob at PythonRDD.scala:181, took 125.963243 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pair each (in order) word in the two sentences\n",
    "word_pairs = joined_filtered_same_length.flatMap(lambda x: zip(x[1][0].split(), x[1][1].split()))\n",
    "\n",
    "print(\"\\nSample of word pairs RDD:\")\n",
    "print(word_pairs.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a02b92-1500-4d74-bf6b-75a87ad624ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use reduce to count the number of occurrences of the word-translation-pairs\n",
    "word_pair_counts = word_pairs.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7633bdc-eb02-4044-a3eb-d0bb0035c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:39:07 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_67839/2001616106.py:3\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Registering RDD 53 (reduceByKey at /tmp/ipykernel_67839/3995230143.py:2) as input to shuffle 5\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Got job 25 (takeOrdered at /tmp/ipykernel_67839/2001616106.py:3) with 5 output partitions\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Final stage: ResultStage 40 (takeOrdered at /tmp/ipykernel_67839/2001616106.py:3)\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 39)\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Submitting ShuffleMapStage 39 (PairwiseRDD[53] at reduceByKey at /tmp/ipykernel_67839/3995230143.py:2), which has no missing parents\n",
      "24/03/14 11:39:07 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 16.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:39:07 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 413.4 MiB)\n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on host-192-168-2-72-de1:10005 (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:39:07 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:39:07 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 39 (PairwiseRDD[53] at reduceByKey at /tmp/ipykernel_67839/3995230143.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/14 11:39:07 INFO TaskSchedulerImpl: Adding task set 39.0 with 5 tasks resource profile 0\n",
      "24/03/14 11:39:07 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 54) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:39:07 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 55) (192.168.2.250, executor 2, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:39:07 INFO TaskSetManager: Starting task 2.0 in stage 39.0 (TID 56) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:39:07 INFO TaskSetManager: Starting task 3.0 in stage 39.0 (TID 57) (192.168.2.250, executor 1, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:39:07 INFO TaskSetManager: Starting task 4.0 in stage 39.0 (TID 58) (192.168.2.250, executor 2, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Removed broadcast_31_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.8 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 192.168.2.250:10007 in memory (size: 7.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:39:07 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.2.250:10007 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:44:09 INFO TaskSetManager: Finished task 3.0 in stage 39.0 (TID 57) in 301770 ms on 192.168.2.250 (executor 1) (1/5)\n",
      "24/03/14 11:44:13 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 55) in 305985 ms on 192.168.2.250 (executor 2) (2/5)\n",
      "24/03/14 11:44:14 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 54) in 307164 ms on 192.168.2.250 (executor 1) (3/5)\n",
      "24/03/14 11:44:21 INFO TaskSetManager: Finished task 4.0 in stage 39.0 (TID 58) in 314433 ms on 192.168.2.250 (executor 2) (4/5)\n",
      "24/03/14 11:44:25 INFO TaskSetManager: Finished task 2.0 in stage 39.0 (TID 56) in 317788 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/14 11:44:25 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:44:25 INFO DAGScheduler: ShuffleMapStage 39 (reduceByKey at /tmp/ipykernel_67839/3995230143.py:2) finished in 317.809 s\n",
      "24/03/14 11:44:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 11:44:25 INFO DAGScheduler: running: Set()\n",
      "24/03/14 11:44:25 INFO DAGScheduler: waiting: Set(ResultStage 40)\n",
      "24/03/14 11:44:25 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 11:44:25 INFO DAGScheduler: Submitting ResultStage 40 (PythonRDD[56] at takeOrdered at /tmp/ipykernel_67839/2001616106.py:3), which has no missing parents\n",
      "24/03/14 11:44:25 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 11.2 KiB, free 413.4 MiB)\n",
      "24/03/14 11:44:25 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 413.4 MiB)\n",
      "24/03/14 11:44:25 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/14 11:44:25 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/14 11:44:25 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 40 (PythonRDD[56] at takeOrdered at /tmp/ipykernel_67839/2001616106.py:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/14 11:44:25 INFO TaskSchedulerImpl: Adding task set 40.0 with 5 tasks resource profile 0\n",
      "24/03/14 11:44:25 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 59) (192.168.2.250, executor 2, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:44:25 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 60) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:44:25 INFO TaskSetManager: Starting task 2.0 in stage 40.0 (TID 61) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:44:25 INFO TaskSetManager: Starting task 3.0 in stage 40.0 (TID 62) (192.168.2.250, executor 2, partition 3, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:44:25 INFO TaskSetManager: Starting task 4.0 in stage 40.0 (TID 63) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7437 bytes) \n",
      "24/03/14 11:44:25 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:44:25 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.2.250:10007 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:44:25 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/14 11:44:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:37450\n",
      "24/03/14 11:44:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:52638\n",
      "24/03/14 11:44:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:48960\n",
      "24/03/14 11:44:59 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 59) in 34006 ms on 192.168.2.250 (executor 2) (1/5)\n",
      "24/03/14 11:44:59 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 60) in 34048 ms on 192.168.2.250 (executor 0) (2/5)\n",
      "24/03/14 11:44:59 INFO TaskSetManager: Finished task 3.0 in stage 40.0 (TID 62) in 34425 ms on 192.168.2.250 (executor 2) (3/5)\n",
      "24/03/14 11:45:00 INFO TaskSetManager: Finished task 4.0 in stage 40.0 (TID 63) in 35123 ms on 192.168.2.250 (executor 0) (4/5)\n",
      "[Stage 40:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the most frequently occurring word pairs:\n",
      "('the', 'att'): 130356\n",
      "('the', 'och'): 102795\n",
      "('the', 'i'): 80330\n",
      "('the', 'det'): 70555\n",
      "('the', 'som'): 69556\n",
      "('the', 'för'): 69335\n",
      "('of', 'att'): 61855\n",
      "('to', 'att'): 57168\n",
      "('the', 'av'): 56353\n",
      "('the', 'är'): 52917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:45:00 INFO TaskSetManager: Finished task 2.0 in stage 40.0 (TID 61) in 35606 ms on 192.168.2.250 (executor 1) (5/5)\n",
      "24/03/14 11:45:00 INFO DAGScheduler: ResultStage 40 (takeOrdered at /tmp/ipykernel_67839/2001616106.py:3) finished in 35.624 s\n",
      "24/03/14 11:45:00 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "24/03/14 11:45:00 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 11:45:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "24/03/14 11:45:00 INFO DAGScheduler: Job 25 finished: takeOrdered at /tmp/ipykernel_67839/2001616106.py:3, took 353.453596 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print some of the most frequently occurring pairs of words\n",
    "MOST_FREQUENT = 10\n",
    "most_frequent_pairs = word_pair_counts.takeOrdered(MOST_FREQUENT, key=lambda x: -x[1])\n",
    "\n",
    "print(\"Some of the most frequently occurring word pairs:\")\n",
    "for pair, count in most_frequent_pairs:\n",
    "    print(f\"{pair}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63115c-f461-4075-9c32-c691a85cd589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The translation is not very accurate and none of the translations are correct. We also see the word \"The\" \n",
    "# being translated into 8 of the Swedish words and \"att\" into 3 of the English words. I think this is due to some \n",
    "# fundamental differences between the two languages. In Sweden, we don't alter the word when we use it in its definite \n",
    "# form whereas in English they add the word \"the\" in front of the word. For example: \n",
    "#\n",
    "# Swedish | English | definite Swedish | definite English\n",
    "# --------------------------------------------------------\n",
    "#  skog   |  forest |      skogen      |    the forest\n",
    "#  boll   |  ball   |      bollen      |    the ball\n",
    "#\n",
    "# Therefore \"the\" will take the place of many different words in Swedish. The result might have been better if we \n",
    "# had filtered out all of the most common words from both as they are usually words to describe other words. Such as \n",
    "# \"THE ball\", and \"THE forest\".\n",
    "# However, I don't think the result would have been much better as the word order in Swedish and English differs from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ce63e2d-6d17-477e-aeb0-c30394406e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 12:14:42 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/14 12:14:42 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-72-de1:4040\n",
      "24/03/14 12:14:42 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/14 12:14:42 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/14 12:14:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/14 12:14:42 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/14 12:14:42 INFO BlockManager: BlockManager stopped\n",
      "24/03/14 12:14:42 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/14 12:14:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/14 12:14:42 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14232871-c041-47a9-93ba-94143e60c7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
