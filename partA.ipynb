{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04162d18-0f55-47a3-8df8-9abaf34f7480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/12 21:53:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"Assignment-3-Axel-Sigstam\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d6fec0-20d0-493e-807b-19e19cc8f949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 413.7 MiB)\n",
      "24/03/12 21:54:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 413.7 MiB)\n",
      "24/03/12 21:54:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-72-de1:10005 (size: 32.6 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:54:04 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/12 21:54:05 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/12 21:54:05 INFO NetworkTopology: Adding a new node: /default-rack/192.168.2.250:9866\n",
      "24/03/12 21:54:05 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:54:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 413.7 MiB)\n",
      "24/03/12 21:54:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 413.7 MiB)\n",
      "24/03/12 21:54:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-72-de1:10005 (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:54:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:54:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:54:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240312215334-0077/0 on worker-20240228152247-192.168.2.250-40823 (192.168.2.250:40823) with 2 core(s)\n",
      "24/03/12 21:54:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20240312215334-0077/0 on hostPort 192.168.2.250:40823 with 2 core(s), 1024.0 MiB RAM\n",
      "24/03/12 21:54:06 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 21:54:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240312215334-0077/0 is now RUNNING\n",
      "24/03/12 21:54:08 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:47188) with ID 0,  ResourceProfileId 0\n",
      "24/03/12 21:54:08 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/03/12 21:54:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10005 with 366.3 MiB RAM, BlockManagerId(0, 192.168.2.250, 10005, None)\n",
      "24/03/12 21:54:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:54:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:54:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2216 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:54:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:11 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52749\n",
      "24/03/12 21:54:11 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 5.962 s\n",
      "24/03/12 21:54:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/12 21:54:11 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 6.054953 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Resumption of the session'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "## A1 ##\n",
    "########\n",
    "\n",
    "# read a file from the HDFS\n",
    "linesEn = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.en')\n",
    "linesEn.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b02573-1085-43b3-91e0-54b829022122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-72-de1:10005 (size: 32.6 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:54:24 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/12 21:54:24 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/12 21:54:24 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:54:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-72-de1:10005 (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:54:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:54:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:54:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:54:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 119 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:54:24 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:181) finished in 0.146 s\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/12 21:54:24 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:181, took 0.159594 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Återupptagande av sessionen'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesSv = spark_context.textFile('hdfs://192.168.2.250:9000/europarl/europarl-v7.sv-en.sv')\n",
    "linesSv.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe0c4e9-5cae-4e14-85c5-495f314d39e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 20:33:04 INFO SparkContext: Starting job: count at /tmp/ipykernel_24907/212818385.py:4\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Got job 7 (count at /tmp/ipykernel_24907/212818385.py:4) with 2 output partitions\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Final stage: ResultStage 7 (count at /tmp/ipykernel_24907/212818385.py:4)\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[15] at count at /tmp/ipykernel_24907/212818385.py:4), which has no missing parents\n",
      "24/03/12 20:33:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 9.1 KiB, free 412.9 MiB)\n",
      "24/03/12 20:33:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 412.9 MiB)\n",
      "24/03/12 20:33:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:33:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 20:33:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[15] at count at /tmp/ipykernel_24907/212818385.py:4) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 20:33:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "24/03/12 20:33:04 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10) (192.168.2.250, executor 2, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/12 20:33:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 20:33:05 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 20:33:05 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 1071 ms on 192.168.2.250 (executor 2) (1/2)\n",
      "24/03/12 20:33:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 11) (192.168.2.250, executor 2, partition 0, ANY, 7690 bytes) \n",
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in English transcripts: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 20:33:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 11) in 1032 ms on 192.168.2.250 (executor 2) (2/2)\n",
      "24/03/12 20:33:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/12 20:33:09 INFO DAGScheduler: ResultStage 7 (count at /tmp/ipykernel_24907/212818385.py:4) finished in 4.953 s\n",
      "24/03/12 20:33:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 20:33:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/12 20:33:09 INFO DAGScheduler: Job 7 finished: count at /tmp/ipykernel_24907/212818385.py:4, took 4.960372 s\n",
      "24/03/12 20:33:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:33:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:33:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "# We notice the same first line in both languages. Good!\n",
    "\n",
    "# Count the number of lines in the English transcript\n",
    "line_count_en = linesEn.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of lines in English transcripts:\", line_count_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b7e85f-9cb4-4436-b7fa-774a53a4a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 20:33:53 INFO SparkContext: Starting job: count at /tmp/ipykernel_24907/3777411404.py:5\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Got job 9 (count at /tmp/ipykernel_24907/3777411404.py:5) with 2 output partitions\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Final stage: ResultStage 9 (count at /tmp/ipykernel_24907/3777411404.py:5)\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[17] at count at /tmp/ipykernel_24907/3777411404.py:5), which has no missing parents\n",
      "24/03/12 20:33:53 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.1 KiB, free 412.9 MiB)\n",
      "24/03/12 20:33:53 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 412.9 MiB)\n",
      "24/03/12 20:33:53 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:33:53 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 20:33:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (PythonRDD[17] at count at /tmp/ipykernel_24907/3777411404.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 20:33:53 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "24/03/12 20:33:53 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 14) (192.168.2.250, executor 2, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/12 20:33:53 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 20:33:54 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 14) in 944 ms on 192.168.2.250 (executor 2) (1/2)\n",
      "24/03/12 20:33:54 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 20:33:56 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 15) (192.168.2.250, executor 2, partition 0, ANY, 7690 bytes) \n",
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in Swedish transcripts: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 20:33:57 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 15) in 925 ms on 192.168.2.250 (executor 2) (2/2)\n",
      "24/03/12 20:33:57 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/12 20:33:57 INFO DAGScheduler: ResultStage 9 (count at /tmp/ipykernel_24907/3777411404.py:5) finished in 4.058 s\n",
      "24/03/12 20:33:57 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 20:33:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/03/12 20:33:57 INFO DAGScheduler: Job 9 finished: count at /tmp/ipykernel_24907/3777411404.py:5, took 4.074596 s\n",
      "24/03/12 20:34:27 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 2\n",
      "24/03/12 20:34:27 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 2\n",
      "24/03/12 20:34:27 INFO ExecutorAllocationManager: Executors 2 removed due to idle timeout.\n",
      "24/03/12 20:34:32 INFO TaskSchedulerImpl: Executor 2 on 192.168.2.250 killed by driver.\n",
      "24/03/12 20:34:32 INFO DAGScheduler: Executor lost: 2 (epoch 2)\n",
      "24/03/12 20:34:32 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.\n",
      "24/03/12 20:34:32 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 192.168.2.250, 10005, None)\n",
      "24/03/12 20:34:32 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor\n",
      "24/03/12 20:34:32 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 2)\n",
      "24/03/12 20:34:32 INFO ExecutorMonitor: Executor 2 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 3, unexpectedly exited: 0).\n"
     ]
    }
   ],
   "source": [
    "# We get:\n",
    "    # Number of lines in English transcripts: 1862234\n",
    "\n",
    "# Count the number of lines in the Swedish transcript\n",
    "line_count_sv = linesEn.count()\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of lines in Swedish transcripts:\", line_count_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6753ffcd-594f-4ff1-a42a-9e78124ef1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line counts are the same for both languages.\n"
     ]
    }
   ],
   "source": [
    "# We got:\n",
    "    # Number of lines in Swedish transcripts: 1862234\n",
    "\n",
    "# Verify if line counts are the same for both languages\n",
    "if line_count_en == line_count_sv:\n",
    "    print(\"Line counts are the same for both languages.\")\n",
    "else:\n",
    "    print(\"Line counts are different for both languages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f027d645-29cb-475c-be02-a089650bc277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for English transcripts RDD: 2\n",
      "Number of partitions for Swedish transcripts RDD: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 20:36:18 INFO BlockManagerInfo: Removed broadcast_11_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:36:18 INFO BlockManagerInfo: Removed broadcast_12_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 20:36:18 INFO BlockManagerInfo: Removed broadcast_13_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.5 KiB, free: 413.8 MiB)\n"
     ]
    }
   ],
   "source": [
    "part_en = linesEn.getNumPartitions()\n",
    "part_sv = linesSv.getNumPartitions()\n",
    "\n",
    "# Print the number of partitions for each RDD\n",
    "print(\"Number of partitions for English transcripts RDD:\", part_en)\n",
    "print(\"Number of partitions for Swedish transcripts RDD:\", part_sv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b766f1e-c855-47fe-8aa3-797ef61d4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed English transcript sample:\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.']\n",
      "['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.']\n",
      "['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.']\n",
      "['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.']\n",
      "['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.']\n",
      "['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)']\n",
      "['madam', 'president,', 'on', 'a', 'point', 'of', 'order.']\n",
      "['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.']\n",
      "['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']\n",
      "\n",
      "Preprocessed Swedish transcript sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:39 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.5 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:54:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 86 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:181) finished in 0.115 s\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:181, took 0.127197 s\n",
      "24/03/12 21:54:39 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.5 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:54:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 66 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:181) finished in 0.087 s\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:181, took 0.100185 s\n",
      "24/03/12 21:54:39 INFO SparkContext: Starting job: count at /tmp/ipykernel_28081/2539811547.py:26\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Got job 4 (count at /tmp/ipykernel_28081/2539811547.py:26) with 2 output partitions\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Final stage: ResultStage 4 (count at /tmp/ipykernel_28081/2539811547.py:26)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[8] at count at /tmp/ipykernel_28081/2539811547.py:26), which has no missing parents\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.6 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[8] at count at /tmp/ipykernel_28081/2539811547.py:26) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:54:39 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:54:39 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10005 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-72-de1:10005 in memory (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:54:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['återupptagande', 'av', 'sessionen']\n",
      "['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.']\n",
      "['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.']\n",
      "['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.']\n",
      "['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.']\n",
      "['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.']\n",
      "['(parlamentet', 'höll', 'en', 'tyst', 'minut.)']\n",
      "['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.']\n",
      "['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.']\n",
      "['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:40 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 21:54:41 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 2102 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/12 21:54:43 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:45 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 2293 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/12 21:54:45 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:45 INFO DAGScheduler: ResultStage 4 (count at /tmp/ipykernel_28081/2539811547.py:26) finished in 6.271 s\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Job 4 finished: count at /tmp/ipykernel_28081/2539811547.py:26, took 6.275603 s\n",
      "24/03/12 21:54:45 INFO SparkContext: Starting job: count at /tmp/ipykernel_28081/2539811547.py:27\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Got job 5 (count at /tmp/ipykernel_28081/2539811547.py:27) with 3 output partitions\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Final stage: ResultStage 5 (count at /tmp/ipykernel_28081/2539811547.py:27)\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[9] at count at /tmp/ipykernel_28081/2539811547.py:27), which has no missing parents\n",
      "24/03/12 21:54:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.6 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:45 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:45 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (PythonRDD[9] at count at /tmp/ipykernel_28081/2539811547.py:27) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:54:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:54:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:54:45 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (192.168.2.250, executor 0, partition 1, ANY, 7690 bytes) \n",
      "24/03/12 21:54:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240312215334-0077/1 on worker-20240228152247-192.168.2.250-40823 (192.168.2.250:40823) with 2 core(s)\n",
      "24/03/12 21:54:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20240312215334-0077/1 on hostPort 192.168.2.250:40823 with 2 core(s), 1024.0 MiB RAM\n",
      "24/03/12 21:54:46 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2 for resource profile id: 0)\n",
      "24/03/12 21:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240312215334-0077/1 is now RUNNING\n",
      "24/03/12 21:54:48 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 8) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/12 21:54:48 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 3027 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/12 21:54:48 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 3061 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:54:49 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 8) in 406 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/12 21:54:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:54:49 INFO DAGScheduler: ResultStage 5 (count at /tmp/ipykernel_28081/2539811547.py:27) finished in 3.458 s\n",
      "24/03/12 21:54:49 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:54:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/12 21:54:49 INFO DAGScheduler: Job 5 finished: count at /tmp/ipykernel_28081/2539811547.py:27, took 3.466624 s\n",
      "24/03/12 21:54:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10005 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:54:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.9 MiB)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'line_count_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m count_processed_lines_en \u001b[38;5;241m=\u001b[39m processed_lines_en\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     27\u001b[0m count_processed_lines_sv \u001b[38;5;241m=\u001b[39m processed_lines_sv\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_processed_lines_en \u001b[38;5;241m==\u001b[39m \u001b[43mline_count_en\u001b[49m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish line count match after preprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'line_count_en' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:46794) with ID 1,  ResourceProfileId 0\n",
      "24/03/12 21:54:49 INFO ExecutorMonitor: New executor 1 has registered (new total is 2)\n",
      "24/03/12 21:54:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10006 with 366.3 MiB RAM, BlockManagerId(1, 192.168.2.250, 10006, None)\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A2 ##\n",
    "########\n",
    "\n",
    "# Function to preprocess text: lowercase and tokenize\n",
    "def preprocess_text(line):\n",
    "    # Lowercase the text\n",
    "    line = line.lower()\n",
    "    # Tokenize the text (split on space)\n",
    "    tokens = line.split()\n",
    "    return tokens\n",
    "\n",
    "processed_lines_en = linesEn.map(preprocess_text)\n",
    "processed_lines_sv = linesSv.map(preprocess_text)\n",
    "\n",
    "# Collect and print a few preprocessed lines from each RDD for verification\n",
    "print(\"Preprocessed English transcript sample:\")\n",
    "for line in processed_lines_en.take(10):\n",
    "    print(line)\n",
    "\n",
    "print(\"\\nPreprocessed Swedish transcript sample:\")\n",
    "for line in processed_lines_sv.take(10):\n",
    "    print(line)\n",
    "\n",
    "# Verify that line count are the same after preprocessing\n",
    "count_processed_lines_en = processed_lines_en.count()\n",
    "count_processed_lines_sv = processed_lines_sv.count()\n",
    "\n",
    "if count_processed_lines_en == line_count_en:\n",
    "    print(\"English line count match after preprocessing.\")\n",
    "else:\n",
    "    print(\"Warning! English line count doesn't match after preprocessing.\")\n",
    "\n",
    "if count_processed_lines_sv == line_count_sv:\n",
    "    print(\"Swedish line count match after preprocessing.\")\n",
    "else:\n",
    "    print(\"Warning! Swedish line count doesn't match after preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe359492-dea8-4699-a94b-6d72aab0e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both languages have matching line count after preprocessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63aabc1d-a3b7-4a5e-8e6a-73a736fc1b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:54:58 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_28081/92222483.py:12\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Registering RDD 11 (reduceByKey at /tmp/ipykernel_28081/92222483.py:12) as input to shuffle 0\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Got job 6 (sortBy at /tmp/ipykernel_28081/92222483.py:12) with 2 output partitions\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Final stage: ResultStage 7 (sortBy at /tmp/ipykernel_28081/92222483.py:12)\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[11] at reduceByKey at /tmp/ipykernel_28081/92222483.py:12), which has no missing parents\n",
      "24/03/12 21:54:58 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:58 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:54:58 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:54:58 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:54:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (PairwiseRDD[11] at reduceByKey at /tmp/ipykernel_28081/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:54:58 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:54:58 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7679 bytes) \n",
      "24/03/12 21:54:59 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:54:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:54:59 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 21:55:02 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 10) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/03/12 21:55:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10005 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:16 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 17776 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/12 21:55:17 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 10) in 15060 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/12 21:55:17 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:17 INFO DAGScheduler: ShuffleMapStage 6 (reduceByKey at /tmp/ipykernel_28081/92222483.py:12) finished in 18.709 s\n",
      "24/03/12 21:55:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/12 21:55:17 INFO DAGScheduler: running: Set()\n",
      "24/03/12 21:55:17 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "24/03/12 21:55:17 INFO DAGScheduler: failed: Set()\n",
      "24/03/12 21:55:17 INFO DAGScheduler: Submitting ResultStage 7 (PythonRDD[14] at sortBy at /tmp/ipykernel_28081/92222483.py:12), which has no missing parents\n",
      "24/03/12 21:55:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (PythonRDD[14] at sortBy at /tmp/ipykernel_28081/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:55:17 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:55:17 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 11) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:17 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 12) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10005 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:47188\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.2.250:10006 (size: 6.7 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10005 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Removed broadcast_8_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.250:46794\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.2.250:10006 in memory (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.8 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:55:17 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.2.250:10005 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 12) in 368 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 11) in 378 ms on 192.168.2.250 (executor 1) (2/2)\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:18 INFO DAGScheduler: ResultStage 7 (sortBy at /tmp/ipykernel_28081/92222483.py:12) finished in 0.396 s\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Job 6 finished: sortBy at /tmp/ipykernel_28081/92222483.py:12, took 19.179565 s\n",
      "24/03/12 21:55:18 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_28081/92222483.py:12\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Got job 7 (sortBy at /tmp/ipykernel_28081/92222483.py:12) with 2 output partitions\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Final stage: ResultStage 9 (sortBy at /tmp/ipykernel_28081/92222483.py:12)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[15] at sortBy at /tmp/ipykernel_28081/92222483.py:12), which has no missing parents\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 11.1 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:18 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (PythonRDD[15] at sortBy at /tmp/ipykernel_28081/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 13) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:18 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 14) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 13) in 238 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 14) in 247 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:18 INFO DAGScheduler: ResultStage 9 (sortBy at /tmp/ipykernel_28081/92222483.py:12) finished in 0.266 s\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Job 7 finished: sortBy at /tmp/ipykernel_28081/92222483.py:12, took 0.273293 s\n",
      "24/03/12 21:55:18 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Registering RDD 17 (sortBy at /tmp/ipykernel_28081/92222483.py:12) as input to shuffle 1\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Got job 8 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Final stage: ResultStage 12 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting ShuffleMapStage 11 (PairwiseRDD[17] at sortBy at /tmp/ipykernel_28081/92222483.py:12), which has no missing parents\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.9 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:18 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (PairwiseRDD[17] at sortBy at /tmp/ipykernel_28081/92222483.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 21:55:18 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 16) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10005 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.2.250:10006 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 333 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 16) in 370 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:18 INFO DAGScheduler: ShuffleMapStage 11 (sortBy at /tmp/ipykernel_28081/92222483.py:12) finished in 0.387 s\n",
      "24/03/12 21:55:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/12 21:55:18 INFO DAGScheduler: running: Set()\n",
      "24/03/12 21:55:18 INFO DAGScheduler: waiting: Set(ResultStage 12)\n",
      "24/03/12 21:55:18 INFO DAGScheduler: failed: Set()\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting ResultStage 12 (PythonRDD[20] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.9 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.3 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.250:10006 in memory (size: 6.7 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.2.250:10005 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (PythonRDD[20] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:55:18 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:55:18 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 17) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.2.250:10005 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.250:47188\n",
      "24/03/12 21:55:19 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 17) in 146 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:55:19 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:19 INFO DAGScheduler: ResultStage 12 (runJob at PythonRDD.scala:181) finished in 0.172 s\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Job 8 finished: runJob at PythonRDD.scala:181, took 0.578473 s\n",
      "24/03/12 21:55:19 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_28081/92222483.py:19\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Registering RDD 22 (reduceByKey at /tmp/ipykernel_28081/92222483.py:19) as input to shuffle 2\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Got job 9 (sortBy at /tmp/ipykernel_28081/92222483.py:19) with 3 output partitions\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Final stage: ResultStage 14 (sortBy at /tmp/ipykernel_28081/92222483.py:19)\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Submitting ShuffleMapStage 13 (PairwiseRDD[22] at reduceByKey at /tmp/ipykernel_28081/92222483.py:19), which has no missing parents\n",
      "24/03/12 21:55:19 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:19 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:19 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:19 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 13 (PairwiseRDD[22] at reduceByKey at /tmp/ipykernel_28081/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:55:19 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:55:19 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 18) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/03/12 21:55:19 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 19) (192.168.2.250, executor 1, partition 1, ANY, 7679 bytes) \n",
      "24/03/12 21:55:19 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 20) (192.168.2.250, executor 0, partition 2, ANY, 7679 bytes) \n",
      "24/03/12 21:55:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10005 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "24/03/12 21:55:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10006 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:21 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 20) in 2083 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 18) in 14996 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 19) in 15042 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/12 21:55:34 INFO DAGScheduler: ShuffleMapStage 13 (reduceByKey at /tmp/ipykernel_28081/92222483.py:19) finished in 15.062 s\n",
      "24/03/12 21:55:34 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/12 21:55:34 INFO DAGScheduler: running: Set()\n",
      "24/03/12 21:55:34 INFO DAGScheduler: waiting: Set(ResultStage 14)\n",
      "24/03/12 21:55:34 INFO DAGScheduler: failed: Set()\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Submitting ResultStage 14 (PythonRDD[25] at sortBy at /tmp/ipykernel_28081/92222483.py:19), which has no missing parents\n",
      "24/03/12 21:55:34 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 11.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:34 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 413.3 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10005 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.2.250:10006 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:34 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (PythonRDD[25] at sortBy at /tmp/ipykernel_28081/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:55:34 INFO TaskSchedulerImpl: Adding task set 14.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 21) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 22) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 23) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Removed broadcast_11_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.250:10006 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.2.250:10005 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:46794\n",
      "24/03/12 21:55:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 192.168.2.250:47188\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Removed broadcast_12_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.2.250:10005 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 21) in 338 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 22) in 371 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 23) in 411 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/12 21:55:34 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:34 INFO DAGScheduler: ResultStage 14 (sortBy at /tmp/ipykernel_28081/92222483.py:19) finished in 0.441 s\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Job 9 finished: sortBy at /tmp/ipykernel_28081/92222483.py:19, took 15.521410 s\n",
      "24/03/12 21:55:34 INFO SparkContext: Starting job: sortBy at /tmp/ipykernel_28081/92222483.py:19\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Got job 10 (sortBy at /tmp/ipykernel_28081/92222483.py:19) with 3 output partitions\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Final stage: ResultStage 16 (sortBy at /tmp/ipykernel_28081/92222483.py:19)\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Submitting ResultStage 16 (PythonRDD[26] at sortBy at /tmp/ipykernel_28081/92222483.py:19), which has no missing parents\n",
      "24/03/12 21:55:34 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.1 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:34 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on host-192-168-2-72-de1:10005 (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:34 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 16 (PythonRDD[26] at sortBy at /tmp/ipykernel_28081/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:55:34 INFO TaskSchedulerImpl: Adding task set 16.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 24) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 25) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 26) (192.168.2.250, executor 1, partition 2, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10006 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.2.250:10005 (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 26) in 393 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 25) in 399 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 24) in 417 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: ResultStage 16 (sortBy at /tmp/ipykernel_28081/92222483.py:19) finished in 0.437 s\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Job 10 finished: sortBy at /tmp/ipykernel_28081/92222483.py:19, took 0.446293 s\n",
      "24/03/12 21:55:35 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Registering RDD 28 (sortBy at /tmp/ipykernel_28081/92222483.py:19) as input to shuffle 3\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Final stage: ResultStage 19 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Submitting ShuffleMapStage 18 (PairwiseRDD[28] at sortBy at /tmp/ipykernel_28081/92222483.py:19), which has no missing parents\n",
      "24/03/12 21:55:35 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 12.0 KiB, free 413.3 MiB)\n",
      "24/03/12 21:55:35 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 413.3 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.250:10005 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.2.250:10006 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.3 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:35 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 18 (PairwiseRDD[28] at sortBy at /tmp/ipykernel_28081/92222483.py:19) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Adding task set 18.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 27) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 21:55:35 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 28) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 21:55:35 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 29) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10006 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.2.250:10005 (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10006 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.2.250:10005 in memory (size: 6.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on host-192-168-2-72-de1:10005 in memory (size: 6.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10006 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.2.250:10005 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 28) in 537 ms on 192.168.2.250 (executor 1) (1/3)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 29) in 549 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 27) in 557 ms on 192.168.2.250 (executor 0) (3/3)\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:35 INFO DAGScheduler: ShuffleMapStage 18 (sortBy at /tmp/ipykernel_28081/92222483.py:19) finished in 0.582 s\n",
      "24/03/12 21:55:35 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/12 21:55:35 INFO DAGScheduler: running: Set()\n",
      "24/03/12 21:55:35 INFO DAGScheduler: waiting: Set(ResultStage 19)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: failed: Set()\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Submitting ResultStage 19 (PythonRDD[31] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:55:35 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.9 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:35 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:35 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (PythonRDD[31] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:55:35 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 30) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 21:55:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.2.250:10006 (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 192.168.2.250:46794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequently Occurring Words in English:\n",
      "the: 3498574\n",
      "of: 1659884\n",
      "to: 1539823\n",
      "and: 1288620\n",
      "in: 1086089\n",
      "that: 797576\n",
      "a: 773812\n",
      "is: 758087\n",
      "for: 534270\n",
      "we: 522879\n",
      "\n",
      "Top 10 Most Frequently Occurring Words in Swedish:\n",
      "att: 1706309\n",
      "och: 1344895\n",
      "i: 1050989\n",
      "det: 924878\n",
      "som: 913302\n",
      "för: 908703\n",
      "av: 738102\n",
      "är: 694389\n",
      "en: 620347\n",
      "vi: 539808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:55:35 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 30) in 180 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/12 21:55:35 INFO DAGScheduler: ResultStage 19 (runJob at PythonRDD.scala:181) finished in 0.199 s\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:35 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "24/03/12 21:55:35 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:181, took 0.793282 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A3 ##\n",
    "########\n",
    "from pyspark.sql.functions import split, explode\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "words_en = linesEn.flatMap(preprocess_text)\n",
    "words_sv = linesSv.flatMap(preprocess_text)\n",
    "\n",
    "# Compute the 10 most frequently occurring words in English\n",
    "english_top_10_words = words_en \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "# Compute the 10 most frequently occurring words in the other language\n",
    "other_language_top_10_words = words_sv \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 Most Frequently Occurring Words in English:\")\n",
    "for word, count in english_top_10_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Frequently Occurring Words in Swedish:\")\n",
    "for word, count in other_language_top_10_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb69b82-71ad-45c4-9153-1d5dad47e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasonable results since all the top 10 words for both languages are among the most common words according to\n",
    "# https://en.wikipedia.org/wiki/Most_common_words_in_English, https://larare.at/svenska/moment/lingvistik/vanligaste_orden_i_svenska_spraket.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73ee2fe-1b28-4ea5-97ad-ceb8b18f97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:55:44 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Got job 12 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6) with 2 output partitions\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Final stage: ResultStage 20 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6)\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Submitting ResultStage 20 (PythonRDD[32] at zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6), which has no missing parents\n",
      "24/03/12 21:55:44 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 8.4 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:44 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:44 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:44 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (PythonRDD[32] at zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/12 21:55:44 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0\n",
      "24/03/12 21:55:44 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 31) (192.168.2.250, executor 1, partition 1, NODE_LOCAL, 7690 bytes) \n",
      "24/03/12 21:55:44 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10006 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:45 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/03/12 21:55:47 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 31) in 2766 ms on 192.168.2.250 (executor 1) (1/2)\n",
      "24/03/12 21:55:48 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 32) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Removed broadcast_17_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.9 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.2.250:10006 in memory (size: 5.9 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.3 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.2.250:10006 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.2.250:10005 in memory (size: 7.3 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:51 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 32) in 2882 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/03/12 21:55:51 INFO DAGScheduler: ResultStage 20 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6) finished in 6.897 s\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:51 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Job 12 finished: zipWithIndex at /tmp/ipykernel_28081/3148246711.py:6, took 6.910467 s\n",
      "24/03/12 21:55:51 INFO SparkContext: Starting job: zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Got job 13 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7) with 3 output partitions\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Final stage: ResultStage 21 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7)\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Submitting ResultStage 21 (PythonRDD[33] at zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7), which has no missing parents\n",
      "24/03/12 21:55:51 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.4 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:51 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:51 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 21 (PythonRDD[33] at zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/03/12 21:55:51 INFO TaskSchedulerImpl: Adding task set 21.0 with 3 tasks resource profile 0\n",
      "24/03/12 21:55:51 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 33) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:55:51 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 34) (192.168.2.250, executor 1, partition 1, ANY, 7690 bytes) \n",
      "24/03/12 21:55:51 INFO TaskSetManager: Starting task 2.0 in stage 21.0 (TID 35) (192.168.2.250, executor 0, partition 2, ANY, 7690 bytes) \n",
      "24/03/12 21:55:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:51 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.2.250:10006 (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:52 INFO TaskSetManager: Finished task 2.0 in stage 21.0 (TID 35) in 537 ms on 192.168.2.250 (executor 0) (1/3)\n",
      "[Stage 21:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of keyed lines in English transcripts RDD:\n",
      "[(0, 'resumption'), (1, 'of'), (2, 'the'), (3, 'session'), (4, 'i')]\n",
      "\n",
      "Sample of keyed lines in transcripts of other language RDD:\n",
      "[(0, 'återupptagande'), (1, 'av'), (2, 'sessionen'), (3, 'jag'), (4, 'förklarar')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:55:55 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 33) in 3575 ms on 192.168.2.250 (executor 0) (2/3)\n",
      "24/03/12 21:55:55 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 34) in 3578 ms on 192.168.2.250 (executor 1) (3/3)\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:55 INFO DAGScheduler: ResultStage 21 (zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7) finished in 3.592 s\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 13 finished: zipWithIndex at /tmp/ipykernel_28081/3148246711.py:7, took 3.598475 s\n",
      "24/03/12 21:55:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Got job 14 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Final stage: ResultStage 22 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Submitting ResultStage 22 (PythonRDD[34] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:55:55 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.3 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:55 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:55 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (PythonRDD[34] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:55:55 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 36) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.2.250:10005 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 36) in 38 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:55 INFO DAGScheduler: ResultStage 22 (runJob at PythonRDD.scala:181) finished in 0.049 s\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 14 finished: runJob at PythonRDD.scala:181, took 0.052465 s\n",
      "24/03/12 21:55:55 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Got job 15 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Final stage: ResultStage 23 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Submitting ResultStage 23 (PythonRDD[35] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:55:55 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 9.3 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:55 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.2.250:10006 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (PythonRDD[35] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_18_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:55 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 37) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.2.250:10005 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_20_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.2.250:10006 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on host-192-168-2-72-de1:10005 in memory (size: 5.2 KiB, free: 413.9 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10006 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:55:55 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 37) in 51 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:55:55 INFO DAGScheduler: ResultStage 23 (runJob at PythonRDD.scala:181) finished in 0.072 s\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:55:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/03/12 21:55:55 INFO DAGScheduler: Job 15 finished: runJob at PythonRDD.scala:181, took 0.075640 s\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "## A4 ##\n",
    "########\n",
    "\n",
    "# Step 1: Key the lines by their line number\n",
    "key_en = words_en.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "key_sv = words_sv.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "print(\"Sample of keyed lines in English transcripts RDD:\")\n",
    "print(key_en.take(5))\n",
    "print(\"\\nSample of keyed lines in transcripts of other language RDD:\")\n",
    "print(key_sv.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01109159-e980-4992-9ee9-032e108b6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of swapped lines in English transcripts RDD:\n",
      "[('resumption', 0), ('of', 1), ('the', 2), ('session', 3), ('i', 4)]\n",
      "\n",
      "Sample of swapped lines in Swedish transcripts RDD:\n",
      "[('återupptagande', 0), ('av', 1), ('sessionen', 2), ('jag', 3), ('förklarar', 4)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:56:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Got job 16 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Final stage: ResultStage 24 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Submitting ResultStage 24 (PythonRDD[36] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:56:01 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:56:01 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:56:01 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:56:01 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (PythonRDD[36] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:56:01 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 38) (192.168.2.250, executor 0, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:56:01 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.2.250:10005 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:56:01 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 38) in 52 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:56:01 INFO DAGScheduler: ResultStage 24 (runJob at PythonRDD.scala:181) finished in 0.077 s\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Job 16 finished: runJob at PythonRDD.scala:181, took 0.086073 s\n",
      "24/03/12 21:56:01 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Got job 17 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Final stage: ResultStage 25 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Submitting ResultStage 25 (PythonRDD[37] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 21:56:01 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.7 KiB, free 413.4 MiB)\n",
      "24/03/12 21:56:01 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 413.4 MiB)\n",
      "24/03/12 21:56:01 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on host-192-168-2-72-de1:10005 (size: 5.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 21:56:01 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (PythonRDD[37] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/03/12 21:56:01 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 39) (192.168.2.250, executor 1, partition 0, ANY, 7690 bytes) \n",
      "24/03/12 21:56:01 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.2.250:10006 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 21:56:01 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 39) in 54 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/03/12 21:56:01 INFO DAGScheduler: ResultStage 25 (runJob at PythonRDD.scala:181) finished in 0.066 s\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 21:56:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/03/12 21:56:01 INFO DAGScheduler: Job 17 finished: runJob at PythonRDD.scala:181, took 0.069181 s\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Swap the key and value\n",
    "swapped_en = key_en.map(lambda x: (x[1], x[0]))\n",
    "swapped_sv = key_sv.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "print(\"\\nSample of swapped lines in English transcripts RDD:\")\n",
    "print(swapped_en.take(5))\n",
    "print(\"\\nSample of swapped lines in Swedish transcripts RDD:\")\n",
    "print(swapped_sv.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0681e591-63bb-410e-ae94-debcc9351151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of joined RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:05:04 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Registering RDD 53 (join at /tmp/ipykernel_28081/552352716.py:2) as input to shuffle 5\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Got job 22 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Final stage: ResultStage 35 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 34)\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Submitting ShuffleMapStage 34 (PairwiseRDD[53] at join at /tmp/ipykernel_28081/552352716.py:2), which has no missing parents\n",
      "24/03/12 22:05:04 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 18.9 KiB, free 413.4 MiB)\n",
      "24/03/12 22:05:04 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 413.4 MiB)\n",
      "24/03/12 22:05:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on host-192-168-2-72-de1:10005 (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:05:04 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:05:04 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 34 (PairwiseRDD[53] at join at /tmp/ipykernel_28081/552352716.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/12 22:05:04 INFO TaskSchedulerImpl: Adding task set 34.0 with 5 tasks resource profile 0\n",
      "24/03/12 22:05:04 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 55) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7788 bytes) \n",
      "24/03/12 22:05:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:05:05 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "24/03/12 22:05:07 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 56) (192.168.2.250, executor 0, partition 0, ANY, 7788 bytes) \n",
      "24/03/12 22:05:07 INFO TaskSetManager: Starting task 2.0 in stage 34.0 (TID 57) (192.168.2.250, executor 2, partition 2, ANY, 7788 bytes) \n",
      "24/03/12 22:05:07 INFO TaskSetManager: Starting task 3.0 in stage 34.0 (TID 58) (192.168.2.250, executor 1, partition 3, ANY, 7788 bytes) \n",
      "24/03/12 22:05:07 INFO TaskSetManager: Starting task 4.0 in stage 34.0 (TID 59) (192.168.2.250, executor 2, partition 4, ANY, 7788 bytes) \n",
      "24/03/12 22:05:07 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:05:07 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 192.168.2.250:10007 (size: 8.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:05:48 INFO TaskSetManager: Finished task 4.0 in stage 34.0 (TID 59) in 40692 ms on 192.168.2.250 (executor 2) (1/5)\n",
      "24/03/12 22:06:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:06:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.2.250:10005 in memory (size: 7.6 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:06:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 192.168.2.250:10006 in memory (size: 7.6 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:09:20 INFO TaskSetManager: Finished task 3.0 in stage 34.0 (TID 58) in 252710 ms on 192.168.2.250 (executor 1) (2/5)\n",
      "24/03/12 22:09:21 INFO TaskSetManager: Finished task 2.0 in stage 34.0 (TID 57) in 253878 ms on 192.168.2.250 (executor 2) (3/5)\n",
      "24/03/12 22:09:57 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 56) in 289627 ms on 192.168.2.250 (executor 0) (4/5)\n",
      "24/03/12 22:10:02 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 55) in 298290 ms on 192.168.2.250 (executor 0) (5/5)\n",
      "24/03/12 22:10:02 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:10:02 INFO DAGScheduler: ShuffleMapStage 34 (join at /tmp/ipykernel_28081/552352716.py:2) finished in 298.315 s\n",
      "24/03/12 22:10:02 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/12 22:10:02 INFO DAGScheduler: running: Set()\n",
      "24/03/12 22:10:02 INFO DAGScheduler: waiting: Set(ResultStage 35)\n",
      "24/03/12 22:10:02 INFO DAGScheduler: failed: Set()\n",
      "24/03/12 22:10:02 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[56] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 22:10:02 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 12.6 KiB, free 413.4 MiB)\n",
      "24/03/12 22:10:02 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 413.4 MiB)\n",
      "24/03/12 22:10:02 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:10:02 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:10:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (PythonRDD[56] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 22:10:02 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "24/03/12 22:10:02 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 60) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 22:10:02 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 192.168.2.250:10005 (size: 7.2 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:10:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:47188\n",
      "24/03/12 22:10:06 INFO BlockManagerInfo: Removed broadcast_29_piece0 on host-192-168-2-72-de1:10005 in memory (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:10:06 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 192.168.2.250:10005 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:10:06 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 192.168.2.250:10006 in memory (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:10:06 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 192.168.2.250:10007 in memory (size: 8.8 KiB, free: 366.3 MiB)\n",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1420, ('in', 'de')), (38120, ('anything', 'betänkande,')), (62930, ('in', 'genom')), (126595, ('monetarism', 'övriga')), (133150, ('that', 'det'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:12:15 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 60) in 132923 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 22:12:15 INFO DAGScheduler: ResultStage 35 (runJob at PythonRDD.scala:181) finished in 132.940 s\n",
      "24/03/12 22:12:15 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 22:12:15 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:12:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
      "24/03/12 22:12:15 INFO DAGScheduler: Job 22 finished: runJob at PythonRDD.scala:181, took 431.278257 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 3: Join the two RDDs together according to the line number key\n",
    "joined = key_en.join(key_sv)\n",
    "\n",
    "print(\"\\nSample of joined RDD:\")\n",
    "print(joined.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a66911-217e-4e19-a42e-ada7ca2c5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (excluding empty/missing sentences):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:12:42 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Final stage: ResultStage 37 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Submitting ResultStage 37 (PythonRDD[57] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 22:12:42 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 13.2 KiB, free 413.4 MiB)\n",
      "24/03/12 22:12:42 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 413.4 MiB)\n",
      "24/03/12 22:12:42 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:12:42 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:12:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (PythonRDD[57] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 22:12:42 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "24/03/12 22:12:42 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 61) (192.168.2.250, executor 2, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 22:12:42 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 192.168.2.250:10007 (size: 7.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:12:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:41284\n",
      "24/03/12 22:14:55 INFO BlockManagerInfo: Removed broadcast_30_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.2 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:14:55 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 192.168.2.250:10005 in memory (size: 7.2 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(80410, ('are', 'på')), (173540, ('one', 'brittiska')), (259860, ('of', 'att')), (282900, ('for', 'nödvändigt')), (371375, ('you', 'utrikes-'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:15:00 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 61) in 137810 ms on 192.168.2.250 (executor 2) (1/1)\n",
      "24/03/12 22:15:00 INFO DAGScheduler: ResultStage 37 (runJob at PythonRDD.scala:181) finished in 137.831 s\n",
      "24/03/12 22:15:00 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 22:15:00 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:15:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "24/03/12 22:15:00 INFO DAGScheduler: Job 23 finished: runJob at PythonRDD.scala:181, took 137.846594 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Filter to exclude line pairs that have an empty/missing “corresponding” sentence\n",
    "joined_filtered = joined.filter(lambda x: all(x[1]))\n",
    "\n",
    "print(\"\\nSample of filtered RDD (excluding empty/missing sentences):\")\n",
    "print(joined_filtered.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422568dd-c60b-4aef-b995-736af097a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (small number of words per sentence):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:15:26 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Got job 24 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Final stage: ResultStage 39 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Submitting ResultStage 39 (PythonRDD[58] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 22:15:26 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 13.8 KiB, free 413.4 MiB)\n",
      "24/03/12 22:15:26 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 413.4 MiB)\n",
      "24/03/12 22:15:26 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:15:26 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (PythonRDD[58] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 22:15:26 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
      "24/03/12 22:15:26 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 62) (192.168.2.250, executor 0, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 22:15:26 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 192.168.2.250:10005 (size: 7.6 KiB, free: 366.2 MiB)\n",
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(66935, ('the', 'och')), (137155, ('procedure', 'biltillverkare')), (155410, ('converter,', 'onsdag')), (178450, ('the', 'dialog')), (271580, ('by', 'är'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:17:42 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 62) in 135788 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/03/12 22:17:42 INFO DAGScheduler: ResultStage 39 (runJob at PythonRDD.scala:181) finished in 135.809 s\n",
      "24/03/12 22:17:42 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 22:17:42 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:17:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
      "24/03/12 22:17:42 INFO DAGScheduler: Job 24 finished: runJob at PythonRDD.scala:181, took 135.821169 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 5: Filter to leave only pairs of sentences with a small number of words per sentence\n",
    "MIN_WORDS_PER_SENTENCE = 5\n",
    "joined_filtered_small = joined_filtered.filter(lambda x: len(x[1][0].split()) <= MIN_WORDS_PER_SENTENCE and len(x[1][1].split()) <= MIN_WORDS_PER_SENTENCE)\n",
    "\n",
    "print(\"\\nSample of filtered RDD (small number of words per sentence):\")\n",
    "print(joined_filtered_small.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2203dd5e-0c78-4350-9422-c5ba85be7b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of filtered RDD (same number of words in each sentence):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:17:54 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Got job 25 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Final stage: ResultStage 41 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Submitting ResultStage 41 (PythonRDD[59] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 22:17:54 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 14.2 KiB, free 413.4 MiB)\n",
      "24/03/12 22:17:54 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 413.4 MiB)\n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:17:54 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:17:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (PythonRDD[59] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 22:17:54 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "24/03/12 22:17:54 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 63) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Removed broadcast_31_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.5 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 192.168.2.250:10007 in memory (size: 7.5 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 192.168.2.250:10005 in memory (size: 7.6 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Removed broadcast_32_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.6 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:17:54 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 192.168.2.250:10006 (size: 7.7 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:17:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.250:46794\n",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19565150, ('this', '1')), (19583405, ('people', 'mark.')), (19658280, ('shared', 'återgå')), (19699575, ('the', 'områdena')), (19746755, ('even', 'för'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:20:05 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 63) in 130954 ms on 192.168.2.250 (executor 1) (1/1)\n",
      "24/03/12 22:20:05 INFO DAGScheduler: ResultStage 41 (runJob at PythonRDD.scala:181) finished in 130.980 s\n",
      "24/03/12 22:20:05 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 22:20:05 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished\n",
      "24/03/12 22:20:05 INFO DAGScheduler: Job 25 finished: runJob at PythonRDD.scala:181, took 130.994362 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6: Filter to leave only pairs of sentences with the same number of words in each sentence\n",
    "joined_filtered_same_length = joined_filtered_small.filter(lambda x: len(x[1][0].split()) == len(x[1][1].split()))\n",
    "\n",
    "print(\"\\nSample of filtered RDD (same number of words in each sentence):\")\n",
    "print(joined_filtered_same_length.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd84ecf-fa61-4179-aeec-24e63342ad9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of word pairs RDD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:20:25 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Got job 26 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Final stage: ResultStage 43 (runJob at PythonRDD.scala:181)\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Submitting ResultStage 43 (PythonRDD[60] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/12 22:20:25 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 14.6 KiB, free 413.4 MiB)\n",
      "24/03/12 22:20:25 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 413.4 MiB)\n",
      "24/03/12 22:20:25 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on host-192-168-2-72-de1:10005 (size: 7.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:20:25 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:20:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (PythonRDD[60] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/12 22:20:25 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
      "24/03/12 22:20:25 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 64) (192.168.2.250, executor 2, partition 0, NODE_LOCAL, 7437 bytes) \n",
      "24/03/12 22:20:25 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 192.168.2.250:10007 (size: 7.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:21:08 INFO BlockManagerInfo: Removed broadcast_33_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.7 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:21:08 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 192.168.2.250:10006 in memory (size: 7.7 KiB, free: 366.2 MiB)\n",
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 'på'), ('one', 'brittiska'), ('of', 'att'), ('for', 'nödvändigt'), ('you', 'utrikes-'), ('parliament,', 'som'), ('not', 'inte'), ('target', 'uppföljningen'), ('the', 'de'), ('the', 'i')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:22:43 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 64) in 137802 ms on 192.168.2.250 (executor 2) (1/1)\n",
      "24/03/12 22:22:43 INFO DAGScheduler: ResultStage 43 (runJob at PythonRDD.scala:181) finished in 137.821 s\n",
      "24/03/12 22:22:43 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/12 22:22:43 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "24/03/12 22:22:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished\n",
      "24/03/12 22:22:43 INFO DAGScheduler: Job 26 finished: runJob at PythonRDD.scala:181, took 137.832036 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 7: Pair each (in order) word in the two sentences\n",
    "word_pairs = joined_filtered_same_length.flatMap(lambda x: zip(x[1][0].split(), x[1][1].split()))\n",
    "\n",
    "print(\"\\nSample of word pairs RDD:\")\n",
    "print(word_pairs.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a02b92-1500-4d74-bf6b-75a87ad624ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:23:17 INFO SparkContext: Starting job: takeOrdered at /tmp/ipykernel_28081/2502667806.py:6\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Registering RDD 62 (reduceByKey at /tmp/ipykernel_28081/2502667806.py:2) as input to shuffle 6\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Got job 27 (takeOrdered at /tmp/ipykernel_28081/2502667806.py:6) with 5 output partitions\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Final stage: ResultStage 46 (takeOrdered at /tmp/ipykernel_28081/2502667806.py:6)\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 45)\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Submitting ShuffleMapStage 45 (PairwiseRDD[62] at reduceByKey at /tmp/ipykernel_28081/2502667806.py:2), which has no missing parents\n",
      "24/03/12 22:23:17 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 16.8 KiB, free 413.4 MiB)\n",
      "24/03/12 22:23:17 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 413.4 MiB)\n",
      "24/03/12 22:23:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on host-192-168-2-72-de1:10005 (size: 8.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:23:17 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/12 22:23:17 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 45 (PairwiseRDD[62] at reduceByKey at /tmp/ipykernel_28081/2502667806.py:2) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/03/12 22:23:17 INFO TaskSchedulerImpl: Adding task set 45.0 with 5 tasks resource profile 0\n",
      "24/03/12 22:23:17 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 65) (192.168.2.250, executor 1, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:23:17 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 66) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:23:17 INFO TaskSetManager: Starting task 2.0 in stage 45.0 (TID 67) (192.168.2.250, executor 2, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:23:17 INFO TaskSetManager: Starting task 3.0 in stage 45.0 (TID 68) (192.168.2.250, executor 1, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:23:17 INFO TaskSetManager: Starting task 4.0 in stage 45.0 (TID 69) (192.168.2.250, executor 0, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:23:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 192.168.2.250:10005 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:23:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 192.168.2.250:10007 (size: 8.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:23:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 192.168.2.250:10006 (size: 8.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:23:34 INFO BlockManagerInfo: Removed broadcast_34_piece0 on host-192-168-2-72-de1:10005 in memory (size: 7.8 KiB, free: 413.8 MiB)\n",
      "24/03/12 22:23:34 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 192.168.2.250:10007 in memory (size: 7.8 KiB, free: 366.3 MiB)\n",
      "24/03/12 22:23:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-72-de1:10005 in memory (size: 4.8 KiB, free: 413.9 MiB)\n",
      "24/03/12 22:23:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.2 MiB)\n",
      "24/03/12 22:24:03 WARN TaskSetManager: Lost task 4.0 in stage 45.0 (TID 69) (192.168.2.250 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 796, in _spill\n",
      "    self.serializer.dump_stream(self.pdata[i].items(), f)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in dump_stream\n",
      "    stream.write(bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:03 INFO TaskSetManager: Starting task 4.1 in stage 45.0 (TID 70) (192.168.2.250, executor 2, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:03 INFO TaskSetManager: Lost task 1.0 in stage 45.0 (TID 66) on 192.168.2.250, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 796, in _spill\n",
      "    self.serializer.dump_stream(self.pdata[i].items(), f)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in dump_stream\n",
      "    stream.write(bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      ") [duplicate 1]\n",
      "24/03/12 22:24:03 INFO TaskSetManager: Starting task 1.1 in stage 45.0 (TID 71) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:04 INFO TaskSetManager: Lost task 2.0 in stage 45.0 (TID 67) on 192.168.2.250, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 796, in _spill\n",
      "    self.serializer.dump_stream(self.pdata[i].items(), f)\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 280, in dump_stream\n",
      "    stream.write(bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      ") [duplicate 2]\n",
      "24/03/12 22:24:04 INFO TaskSetManager: Starting task 2.1 in stage 45.0 (TID 72) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:10 WARN TaskSetManager: Lost task 3.0 in stage 45.0 (TID 68) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/python/1521858/139675305983376/2'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:10 INFO TaskSetManager: Starting task 3.1 in stage 45.0 (TID 73) (192.168.2.250, executor 1, partition 3, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:11 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 65) (192.168.2.250 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/python/1521849/139675305983376/2'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:11 INFO TaskSetManager: Starting task 0.1 in stage 45.0 (TID 74) (192.168.2.250, executor 2, partition 0, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:13 WARN TaskSetManager: Lost task 4.1 in stage 45.0 (TID 70) (192.168.2.250 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-cd2e0b8e-5b45-4c90-a413-2cf7ee2af137/python/1521884'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:13 INFO TaskSetManager: Starting task 4.2 in stage 45.0 (TID 75) (192.168.2.250, executor 2, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:13 WARN TaskSetManager: Lost task 4.2 in stage 45.0 (TID 75) (192.168.2.250 executor 2): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-cd2e0b8e-5b45-4c90-a413-2cf7ee2af137/1a: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:13 INFO TaskSetManager: Starting task 4.3 in stage 45.0 (TID 76) (192.168.2.250, executor 1, partition 4, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:14 WARN TaskSetManager: Lost task 2.1 in stage 45.0 (TID 72) (192.168.2.250 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-d393e8f0-5ea3-444f-af93-abca7ab3273b/python/1521891'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:14 INFO TaskSetManager: Starting task 2.2 in stage 45.0 (TID 77) (192.168.2.250, executor 0, partition 2, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:14 WARN TaskSetManager: Lost task 1.1 in stage 45.0 (TID 71) (192.168.2.250 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "  [Previous line repeated 6 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 4183, in groupByKey\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 299, in mergeCombiners\n",
      "    self._spill()\n",
      "  File \"/home/ubuntu/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/shuffle.py\", line 753, in _spill\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 213, in makedirs\n",
      "    makedirs(head, exist_ok=exist_ok)\n",
      "  File \"/usr/lib/python3.8/os.py\", line 223, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-d393e8f0-5ea3-444f-af93-abca7ab3273b/python/1521883'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:14 INFO TaskSetManager: Starting task 1.2 in stage 45.0 (TID 78) (192.168.2.250, executor 0, partition 1, NODE_LOCAL, 7426 bytes) \n",
      "24/03/12 22:24:17 WARN TaskSetManager: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/12 22:24:17 ERROR TaskSetManager: Task 4 in stage 45.0 failed 4 times; aborting job\n",
      "24/03/12 22:24:17 INFO TaskSchedulerImpl: Cancelling stage 45\n",
      "24/03/12 22:24:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage cancelled: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/12 22:24:17 INFO TaskSchedulerImpl: Stage 45 was cancelled\n",
      "24/03/12 22:24:17 INFO DAGScheduler: ShuffleMapStage 45 (reduceByKey at /tmp/ipykernel_28081/2502667806.py:2) failed in 60.183 s due to Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/12 22:24:17 INFO DAGScheduler: Job 27 failed: takeOrdered at /tmp/ipykernel_28081/2502667806.py:6, took 60.193430 s\n",
      "24/03/12 22:24:17 WARN TaskSetManager: Lost task 2.2 in stage 45.0 (TID 77) (192.168.2.250 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 45:>                                                         (0 + 3) / 5]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 9: Print some of the most frequently occurring pairs of words\u001b[39;00m\n\u001b[1;32m      5\u001b[0m MOST_FREQUENT_PAIRS_TO_PRINT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 6\u001b[0m most_frequent_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mword_pair_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtakeOrdered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMOST_FREQUENT_PAIRS_TO_PRINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2778\u001b[0m, in \u001b[0;36mRDD.takeOrdered\u001b[0;34m(self, num, key)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(a: List[T], b: List[T]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[1;32m   2776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m heapq\u001b[38;5;241m.\u001b[39mnsmallest(num, a \u001b[38;5;241m+\u001b[39m b, key)\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mheapq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsmallest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n\tat java.nio.file.Files.createDirectory(Files.java:674)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 22:24:19 WARN TaskSetManager: Lost task 0.1 in stage 45.0 (TID 74) (192.168.2.250 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/03/12 22:24:20 WARN TaskSetManager: Lost task 3.1 in stage 45.0 (TID 73) (192.168.2.250 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/03/12 22:24:20 WARN TaskSetManager: Lost task 1.2 in stage 45.0 (TID 78) (192.168.2.250 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 45.0 failed 4 times, most recent failure: Lost task 4.3 in stage 45.0 (TID 76) (192.168.2.250 executor 1): java.nio.file.FileSystemException: /tmp/spark-c8e77b16-a188-44a7-a287-f1e3fd73de0b/executor-b6afad53-40cf-4bb9-95f1-d245137b12c1/blockmgr-963d6adb-fef9-42dc-b4b5-eb47cb7d2426/07: No space left on device\n",
      "\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n",
      "\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n",
      "\tat sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)\n",
      "\tat java.nio.file.Files.createDirectory(Files.java:674)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:126)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.$anonfun$getDataFile$2(IndexShuffleBlockResolver.scala:104)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:104)\n",
      "\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.getDataFile(IndexShuffleBlockResolver.scala:66)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.<init>(LocalDiskShuffleMapOutputWriter.java:78)\n",
      "\tat org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents.createMapOutputWriter(LocalDiskShuffleExecutorComponents.java:71)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:138)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/03/12 22:24:20 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n"
     ]
    }
   ],
   "source": [
    "# Step 8: Use reduce to count the number of occurrences of the word-translation-pairs\n",
    "word_pair_counts = word_pairs.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 9: Print some of the most frequently occurring pairs of words\n",
    "MOST_FREQUENT_PAIRS_TO_PRINT = 10\n",
    "most_frequent_pairs = word_pair_counts.takeOrdered(MOST_FREQUENT_PAIRS_TO_PRINT, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4037766c-1a18-42fa-be46-9c8d974e15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the most frequently occurring word pairs:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'most_frequent_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of the most frequently occurring word pairs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair, count \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmost_frequent_pairs\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'most_frequent_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Some of the most frequently occurring word pairs:\")\n",
    "for pair, count in most_frequent_pairs:\n",
    "    print(f\"{pair}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63115c-f461-4075-9c32-c691a85cd589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9e321-d9fd-4607-95be-01e4652bf417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9466d-de94-4110-a99d-011f8a3a2dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0996bc-e348-49d2-894a-8313be3a3207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ce63e2d-6d17-477e-aeb0-c30394406e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/12 21:51:43 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/12 21:51:43 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-72-de1:4040\n",
      "24/03/12 21:51:43 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/12 21:51:43 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/12 21:51:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/12 21:51:43 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/12 21:51:43 INFO BlockManager: BlockManager stopped\n",
      "24/03/12 21:51:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/12 21:51:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/12 21:51:43 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19282cca-3c29-4c58-8014-3c25645c8989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
